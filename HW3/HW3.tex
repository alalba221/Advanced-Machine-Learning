\documentclass[11pt]{article}
\usepackage{amsmath,amsbsy,amssymb,verbatim,fullpage,ifthen,graphicx,bm,amsfonts,amsthm,url}
\usepackage{graphicx}
\usepackage{xcolor}
\newcommand{\mfile}[1]  {{\small \verbatiminput{./#1}}} % Jeff Fessler, input matlab file
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%\newcommand*{\qed}{\hfill\ensuremath{\blacksquare}}%
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\minimize}{\operatorname*{minimize\ }}
\newcommand{\maximize}{\operatorname*{maximize}}
\newcommand{\opdet}[1]{\operatorname{\textbf{det}}\left(#1\right)}
\newcommand{\optr}[1]{\operatorname{\textbf{tr}}\left(#1\right)}
\newcommand{\AnswerDefine}{}
\newcommand{\answer}[2][blue]{\ifdefined\AnswerDefine{\color{#1}\it#2}\fi}
\newcommand{\mtx}[1]{\mathbf{#1}}
\newcommand{\vct}[1]{\mathbf{#1}}
\def \lg       {\langle}
\def \rg       {\rangle}
\def \mA {\mtx{A}}
\def \mF {\mtx{F}}
\def \mG {\mtx{G}}
\def \mI {\mtx{I}}
\def \mJ {\mtx{J}}
\def \mU {\mtx{U}}
\def \mS {\mtx{S}}
\def \mV {\mtx{V}}
\def \mW {\mtx{W}}
\def \mLambda {\mtx{\Lambda}}
\def \mSigma {\mtx{\Sigma}}
\def \mX {\mtx{X}}
\def \mY {\mtx{Y}}
\def \mZ {\mtx{Z}}
\def \zero     {\mathbf{0}}
\def \vzero    {\vct{0}}
\def \vone    {\vct{1}}
\def \va {\vct{a}}
\def \vg {\vct{g}}
\def \vu {\vct{u}}
\def \vv {\vct{v}}
\def \vx {\vct{x}}
\def \vy {\vct{y}}
\def \vz {\vct{z}}
\def \vphi {\vct{\phi}}
\def \vmu {\vct{\mu}}
\def \R {\mathbb{R}}

%\newcommand{\st}{\operatorname*{\ subject\ to\ }}
\usepackage{algorithm,algpseudocode}
\usepackage{xspace}
% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot} \def\st{\emph{s.t}\onedot}
\pagestyle{plain}

\title{{\bf Homework Set 3, CPSC 8420, Fall 2023}} % Change to the appropriate homework number
\author{\Large\underline{Last Name, First Name}}
\date{\textbf{\Large\textcolor{red}{Due 11/17/2023, Friday, 11:59PM EST}}} % put your name in the LastName, FirstName format
%\date{\today}

\begin{document}
\maketitle

\section*{Problem 1}
Considering soft margin SVM, where we have the objective and constraints as follows:
\begin{equation}\label{eq:1}
	\begin{aligned}
		min\;\; &\frac{1}{2}||w||_2^2 +C\sum\limits_{i=1}^{m}\xi_i\\s.t.  \;\; y_i(w^Tx_i + &b)  \geq 1 - \xi_i \;\;(i =1,2,...m)\\\xi_i \geq &0 \;\;(i =1,2,...m)
	\end{aligned}
\end{equation}
Now we formulate another formulation as:
\begin{equation}
	\begin{aligned}
		min\;\; &\frac{1}{2}||w||_2^2 +\frac{C}{2}\sum\limits_{i=1}^{m}\xi_i^2\\s.t.  \;\; y_i(w^Tx_i + &b)  \geq 1 - \xi_i \;\;(i =1,2,...m)
	\end{aligned}
\end{equation}
\begin{enumerate}
	\item Different from Eq. (\ref{eq:1}), we now drop the non-negative constraint for $\xi_i$, please show that optimal value of the objective will be the same when $\xi_i$ constraint is removed.
		The Lagrangian for the object is
		\begin{align}
			L = \frac{1}{2}\omega^T\omega+\frac{C}{2}\sum_{i=1}^{m}\xi_i^2-\sum_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] ; \alpha_i\geq 0
		\end{align}
		then the original problem can be converted to 
		\begin{align}
			\min_{x}\max_{\omega\;\xi\;b}L &= \min_{x}\max_{\omega\;\xi\;b}\frac{1}{2}\omega^T\omega+\frac{C}{2}\sum_{i=1}^{m}\xi_i^2-\sum_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i]\\ \label{eq:minmax}
			&=\min_{x} \left( \max_{\omega\;\xi\;b}\left(\frac{1}{2}\omega^T\omega+
			\frac{C}{2}\sum_{i=1}^{m}\xi_i^2-\sum_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \right) + \max_{\xi}\sum_{i=1}^{m}-\alpha_i\xi_i\right)
			\\
			\text{for } \xi_i<0 &\Rightarrow  \max_{\xi}\sum_{i=1}^{m}-\alpha_i\xi_i = \infty \Rightarrow \max L = \infty \label{eq:less} \\ 
			\text{for } \xi_i\geq 0 &\Rightarrow  \max L < \infty \label{eq:greater}
		\end{align}
		Comnine (\ref{eq:minmax}), (\ref{eq:less}) and (\ref{eq:greater}), we can greater
	\begin{equation}
			\min_{x}\max_{\omega\;\xi\;b}L  = \min_x\left( \overbrace{\max L}^{\xi_i\geq 0},\overbrace{+\infty}^{\xi_i<0}\right) = \min_x\max L
	\end{equation}
	then it proved, the optimization will drop the part for $\xi_i<0$ automatically, so the optimal value of the objectives are the same
	\item What's the generalized Lagrangian of the new soft margin SVM optimization problem?
		\begin{equation}
			L = \frac{1}{2}\omega^T\omega+\frac{C}{2}\sum_{i=1}^{m}\xi_i^2-\sum_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1 + \xi_i] ; \alpha_i\geq 0
		\end{equation}
	\item Now please minimize the Lagrangian with respect to $w, b$, and $\xi$.
		\begin{align}
			\frac{\partial L}{\partial \omega} = 0 \Rightarrow \omega = \sum_{i=1}^{m}\alpha_i y_i x_i\\
			\frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^{m}\alpha_iy_i=0\\
			\frac{\partial L}{\partial \xi} = 0 \Rightarrow \xi_i=\frac{\alpha_i}{C}
		\end{align}
	
	\item What is the dual of this version soft margin SVM optimization problem? (should be similar to Eq. (10) in the slides)
		\begin{align}
				\max_{\alpha}\min_{\omega\;\xi\;b}L(\omega\;\xi\;b\;\alpha) = \\
				\min_{\alpha} \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^T x_i-\sum_{i=1}^{m}\alpha_i+\frac{1}{2}\sum_{i=1}^{m}\frac{\alpha_i^2}{C} \label{eq:newsoft}
		\end{align}
		s.t
		$$\sum_{i=1}^{m}\alpha_iy_i=0\;\;, \alpha_i\geq 0$$
\end{enumerate}

\section*{Problem 2}
Recall vanilla SVM objective:
\begin{equation}
\begin{aligned}
L(w,b,\alpha) = \frac{1}{2}||w||_2^2 - \sum\limits_{i=1}^{m}\alpha_i[y_i(w^Tx_i + b) - 1] \; \quad s.t. \quad \alpha_i \geq 0
\end{aligned}
\end{equation}
If we denote the margin as $\gamma$, and vector $\alpha=[\alpha_1, \alpha_2, \dots, \alpha_m]$, now please show $\gamma^2*\|\alpha\|_1=1$.


Since
\begin{align}
	\frac{\partial L}{\partial \omega} =0 \Rightarrow \omega = \sum_{i=1}^{m}\alpha_iy_ix_i\\
	\frac{\partial L}{\partial b} =0 \Rightarrow \sum_{i=1}^{m}\alpha_iy_i=0\\
	b = y_i-\sum_{j=1}^{m}\alpha_jy_jx_j^Tx_i \label{eq:b}
\end{align}
then mutiply $\sum_{i=1}^{m}\alpha_iy_i$ on both sides of (\ref{eq:b})
then we got
\begin{align}
	\sum_{i=1}^{m}\alpha_iy_ib &= \sum_{i=1}^{m}\alpha_iy_i^2-\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_iy_i\alpha_jy_jx_j^Tx_i\\
	\text{Since } y_i^2=1\; ,\omega = \sum_{i=1}^{m}\alpha_iy_ix_i \;  \sum_{i=1}^{m}\alpha_iy_i=0&\Rightarrow \sum_{i=1}^{m}\alpha_i = ||\omega||^2\\
	\text{by definition } ||\alpha||_1 = \sum_{i=1}^{m}\alpha_i \;\; , \gamma = \frac{1}{||\omega||} &\Rightarrow \gamma^2\times ||\alpha||_1 = 1
\end{align}

\end{document}
